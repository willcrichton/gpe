{"name":"Genetic Polygon Encoding","tagline":"Image compression via polygons and genetic algorithms","body":"## Checkpoint 1.5\r\n\r\nIn the last week, I rewrote the entire polygon renderer from scratch to use both radial gradients as well as antialiasing. I also added the basic polygon compressor. I've been working on:\r\n* Efficiency. The algorithm takes a _long_ time to run, so I'm doing whatever possible to make each set of mutations and fitness evaluations occur as quickly as possible, an iterative process of profiling and tweaking. The most expensive computation is rendering a set of polygons to an image (a la the 418 circle renderer), so I'm considering offloading the rendering to CUDA or something more efficient using Rust's FFI.\r\n* Heuristics. The baseline genetic algorithm just randomly changes colors/vertex positions/etc. based on some threshold (e.g. 1/1500 chance). I'm adding heuristics to the mutations to more closely match the base image, like polygons covering regions with low fitness scores and having a start color equal to the average of the region's color in the base image.\r\n* Constants. As mentioned, each mutation occurs with some frequency, so I'm attempting to find the optimal frequencies as well as optimal population size.\r\n\r\nThe initial incarnation of the algorithm took 900,000 generations and 3 hours to produce an image. I'm using a larger population size and hence run generations less quickly. I haven't taken the 3 hours yet to produce a quality image, but here's an image of the Mona Lisa after ~1k generations and 2 minutes:\r\n\r\n![](images/out3.png)\r\n\r\nI'll update the next checkpoint with a higher quality image.\r\n\r\n## Checkpoint 1\r\n\r\nI'm going to preface this by saying the past two weeks have been utter hell--I've been working from morning to midnight every day trying to finish compilers, do algo, and keep up with interviews and other commitments. By the end of this week, L5 will be done for compilers, and life settles down a bit since interview season is over. Hence, I don't have too much to show for checkpoint 1, but I will work twice as hard before the next checkpoint to stay on schedule.\r\n\r\nThat said, I have a basic Rust system that accepts an image off the command line, parses it, and I have a polygon renderer. I still need to find a way to render polygons with a gradient--this means writing my own polygon renderer instead of using a prebuilt GL-based one. \r\n\r\nThe schedule remains the same--I will have a basic genetic algorithm and a .gpe output file by the end of next week. Likewise, the anticipated deliverable remains the same: I'll have a ready-to-go image compressor and .gpe renderer to show off. If I have time, I'll also build a tool that lets you visualize the population of the genetic algorithm after each iteration.\r\n\r\n## Proposal\r\n\r\n### Summary\r\nI will create a novel lossy image compression algorithm/format which decomposes an image into a small number of polygons using a genetic algorithm.\r\n\r\n### Background\r\nRoger Alsing showed in 2008 the possibility of [representing images with polygons](http://rogeralsing.com/2008/12/07/genetic-programming-evolution-of-mona-lisa/). He applied a genetic algorithm which randomly generates polygons and uses a stochastic selection process to prefer images closest to the one we want to generate. I want to extend this work to both a) be suitable as a new image format and b) generate optimal images as fast as possible.\r\n\r\n### Challenges\r\nFirstly, as far as I can tell, this space hasn't been explored beyond the above blog post and subsequent discussion the topic. There are no references on optimal genetic algorithms for mapping polygons to images. I've dealt in neither genetic algorithms nor image compression before, so I'll have to explore ways to optimize both.\r\n\r\n### Resources\r\nI'll just be using my own machine for the work. If it's suitable, I will draw from the Alsing codebase to provide a starting point for my compression algorithm. I will continue to explore the research space to find other genetic compression algorithms and see if they use any ideas applicable to my project.\r\n\r\n### Goals\r\n**Plan to Achieve**\r\n* Optimize the Alsing algorithm to find better reconstructions faster. Alsing only explored this space in brief, so I think there's definitely room for more work.\r\n* Create a new file format for my compressed images that achieves at least a 2x reduction in size over JPEG without a significant loss in comparative quality. \r\n\r\n**Hope to Achieve**\r\n* Reduce compression times to something actually usable by a consumer.\r\n* Achieve quality greater than or equal to JPEG without trading compression.\r\n\r\n### Schedule\r\n* Friday, November 14: have basic algorithm up and running, i.e. converting the Alsing algorithm to Rust.\r\n* Friday, November 21: compressed image format is done, and images are being saved to .gpe.\r\n* Monday, December 1: research is done and genetic algorithm is as optimized as possible for correctness.\r\n* Monday, December 8: algorithm is further optimized for speed, and benchmarks are created. Rest of the time is spent running tests and doing writeup.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}