<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Genetic Polygon Encoding : Image compression via polygons and genetic algorithms">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Genetic Polygon Encoding</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/willcrichton/gpe">View on GitHub</a>

          <h1 id="project_title">Genetic Polygon Encoding</h1>
          <h2 id="project_tagline">Image compression via polygons and genetic algorithms</h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/willcrichton/gpe/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/willcrichton/gpe/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h2>
<a id="final-writeup" class="anchor" href="#final-writeup" aria-hidden="true"><span class="octicon octicon-link"></span></a>Final Writeup</h2>

<h3>
<a id="summary" class="anchor" href="#summary" aria-hidden="true"><span class="octicon octicon-link"></span></a>Summary</h3>

<p>I created a novel image compression scheme which combines the polygon and pixel bases. For images with less detail and harder edges, I found better compression than JPEG by a factor up to 18x.</p>

<h2>
<a id="checkpoint-2" class="anchor" href="#checkpoint-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Checkpoint 2</h2>

<p>I've ported over the rendering to CUDA, although now that I've figured out Rust's foreign function interface, I might consider finally ending up just doing OpenGL, so there may be on final optimization pass to come on the rendering side.</p>

<p>I ran the compressor on the following images with a 70% quality threshold and got the following images:</p>

<p><img src="images/in4.png" alt=""><img src="images/out4.png" alt=""></p>

<p><img src="images/in5.png" alt=""><img src="images/out5.png" alt=""></p>

<p>I didn't achieve as much in the way of quality as I had hoped, mostly because I've been continuing to focus on optimizations for the past week. I'll post more images this weekend, as I'll be working through the week on creating higher quality output.</p>

<p>My goal over the next 10 days is to improve quality wherever possible. This includes:</p>

<ul>
<li>Multi-resolution sampling</li>
<li>Modifying the compression to include individual pixels</li>
<li>Weighting the addition of new polygons towards uncovered regions of the image</li>
</ul>

<h2>
<a id="checkpoint-15" class="anchor" href="#checkpoint-15" aria-hidden="true"><span class="octicon octicon-link"></span></a>Checkpoint 1.5</h2>

<p>In the last week, I rewrote the entire polygon renderer from scratch to use both radial gradients as well as antialiasing. I also added the basic polygon compressor. I've been working on:</p>

<ul>
<li>Efficiency. The algorithm takes a <em>long</em> time to run, so I'm doing whatever possible to make each set of mutations and fitness evaluations occur as quickly as possible, an iterative process of profiling and tweaking. The most expensive computation is rendering a set of polygons to an image (a la the 418 circle renderer), so I'm considering offloading the rendering to CUDA or something more efficient using Rust's FFI.</li>
<li>Heuristics. The baseline genetic algorithm just randomly changes colors/vertex positions/etc. based on some threshold (e.g. 1/1500 chance). I'm adding heuristics to the mutations to more closely match the base image, like polygons covering regions with low fitness scores and having a start color equal to the average of the region's color in the base image.</li>
<li>Constants. As mentioned, each mutation occurs with some frequency, so I'm attempting to find the optimal frequencies as well as optimal population size.</li>
</ul>

<p>The initial incarnation of the algorithm took 900,000 generations and 3 hours to produce an image. I'm using a larger population size and hence run generations less quickly. I haven't taken the 3 hours yet to produce a quality image, but here's an image of the Mona Lisa after ~1k generations and 2 minutes:</p>

<p><img src="images/out3.png" alt=""></p>

<p>I'll update the next checkpoint with a higher quality image.</p>

<h2>
<a id="checkpoint-1" class="anchor" href="#checkpoint-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Checkpoint 1</h2>

<p>I'm going to preface this by saying the past two weeks have been utter hell--I've been working from morning to midnight every day trying to finish compilers, do algo, and keep up with interviews and other commitments. By the end of this week, L5 will be done for compilers, and life settles down a bit since interview season is over. Hence, I don't have too much to show for checkpoint 1, but I will work twice as hard before the next checkpoint to stay on schedule.</p>

<p>That said, I have a basic Rust system that accepts an image off the command line, parses it, and I have a polygon renderer. I still need to find a way to render polygons with a gradient--this means writing my own polygon renderer instead of using a prebuilt GL-based one. </p>

<p>The schedule remains the same--I will have a basic genetic algorithm and a .gpe output file by the end of next week. Likewise, the anticipated deliverable remains the same: I'll have a ready-to-go image compressor and .gpe renderer to show off. If I have time, I'll also build a tool that lets you visualize the population of the genetic algorithm after each iteration.</p>

<h2>
<a id="proposal" class="anchor" href="#proposal" aria-hidden="true"><span class="octicon octicon-link"></span></a>Proposal</h2>

<h3>
<a id="summary-1" class="anchor" href="#summary-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Summary</h3>

<p>I will create a novel lossy image compression algorithm/format which decomposes an image into a small number of polygons using a genetic algorithm.</p>

<h3>
<a id="background" class="anchor" href="#background" aria-hidden="true"><span class="octicon octicon-link"></span></a>Background</h3>

<p>Roger Alsing showed in 2008 the possibility of <a href="http://rogeralsing.com/2008/12/07/genetic-programming-evolution-of-mona-lisa/">representing images with polygons</a>. He applied a genetic algorithm which randomly generates polygons and uses a stochastic selection process to prefer images closest to the one we want to generate. I want to extend this work to both a) be suitable as a new image format and b) generate optimal images as fast as possible.</p>

<h3>
<a id="challenges" class="anchor" href="#challenges" aria-hidden="true"><span class="octicon octicon-link"></span></a>Challenges</h3>

<p>Firstly, as far as I can tell, this space hasn't been explored beyond the above blog post and subsequent discussion the topic. There are no references on optimal genetic algorithms for mapping polygons to images. I've dealt in neither genetic algorithms nor image compression before, so I'll have to explore ways to optimize both.</p>

<h3>
<a id="resources" class="anchor" href="#resources" aria-hidden="true"><span class="octicon octicon-link"></span></a>Resources</h3>

<p>I'll just be using my own machine for the work. If it's suitable, I will draw from the Alsing codebase to provide a starting point for my compression algorithm. I will continue to explore the research space to find other genetic compression algorithms and see if they use any ideas applicable to my project.</p>

<h3>
<a id="goals" class="anchor" href="#goals" aria-hidden="true"><span class="octicon octicon-link"></span></a>Goals</h3>

<p><strong>Plan to Achieve</strong></p>

<ul>
<li>Optimize the Alsing algorithm to find better reconstructions faster. Alsing only explored this space in brief, so I think there's definitely room for more work.</li>
<li>Create a new file format for my compressed images that achieves at least a 2x reduction in size over JPEG without a significant loss in comparative quality. </li>
</ul>

<p><strong>Hope to Achieve</strong></p>

<ul>
<li>Reduce compression times to something actually usable by a consumer.</li>
<li>Achieve quality greater than or equal to JPEG without trading compression.</li>
</ul>

<h3>
<a id="schedule" class="anchor" href="#schedule" aria-hidden="true"><span class="octicon octicon-link"></span></a>Schedule</h3>

<ul>
<li>Friday, November 14: have basic algorithm up and running, i.e. converting the Alsing algorithm to Rust.</li>
<li>Friday, November 21: compressed image format is done, and images are being saved to .gpe.</li>
<li>Monday, December 1: research is done and genetic algorithm is as optimized as possible for correctness.</li>
<li>Monday, December 8: algorithm is further optimized for speed, and benchmarks are created. Rest of the time is spent running tests and doing writeup.</li>
</ul>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Genetic Polygon Encoding maintained by <a href="https://github.com/willcrichton">willcrichton</a></p>
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
